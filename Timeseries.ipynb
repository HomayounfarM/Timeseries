{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNEDH7Kd/etR9jAMFllkf/C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HomayounfarM/Timeseries/blob/main/Timeseries.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://colab.research.google.com/#create=true&language=r, or this short URL https://colab.to/r"
      ],
      "metadata": {
        "id": "Jta4S-MVZQUt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Me1_2hnCVeFU"
      },
      "outputs": [],
      "source": [
        "install.packages(\"googledrive\")\n",
        "library(\"googledrive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Classical Decomposition Model\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JiT9H9XfZidY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load required libraries\n",
        "library(Kendall)\n",
        "library(tidyverse)\n",
        "library(fpp2) \n",
        "library(forecast) \n",
        "library(tibbletime) \n",
        "library(tsbox) \n",
        "library(gridExtra) \n",
        "library(knitr) \n",
        "library(ggthemes)\n",
        "library(Kendall)"
      ],
      "metadata": {
        "id": "3WHJ31zavNCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#install.packages('Kendall')\n",
        "#install.packages('gridExtra')\n",
        "#install.packages('fpp2')\n",
        "#install.packages('tibbletime')\n",
        "#install.packages('tsbox')\n",
        "#install.packages('ggthemes')"
      ],
      "metadata": {
        "id": "l8YWMsTQy3E0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assuming we have the following time series:"
      ],
      "metadata": {
        "id": "_VNLtH2V5EQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LakeHuron\n",
        "class(LakeHuron)\n",
        "str(LakeHuron)\n",
        "\n",
        "\n",
        "# Convert time series to tibble object\n",
        "LakeHuron_tbl <- data.frame(water_level=as.numeric(LakeHuron), \n",
        "                        year=as.numeric(time(LakeHuron)))\n",
        "\n",
        "class(LakeHuron_tbl)\n",
        "\n",
        "# Plot the data \n",
        "ggplot(LakeHuron_tbl,aes(x=year,y=water_level)) + \n",
        "  ggtitle(\"Lake Huron water levels (1875-1972)\") + \n",
        "  geom_line() + geom_point() +  \n",
        "  ylab(\"Water Level\") + xlab(\"Year\") "
      ],
      "metadata": {
        "id": "wdHdyO4NV3V7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Perform the Mann-Kendall Trend Test\n",
        "MannKendall(LakeHuron)"
      ],
      "metadata": {
        "id": "cytfPAOP3t18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The test statistic is -0.354 and the corresponding two-sided p-value is 2.4718e-07. Because this p-value is less than 0.05, we will reject the null hypothesis of the test and conclude that a trend is present in the data."
      ],
      "metadata": {
        "id": "T5-R2M6L4DJc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So indeed, we have some data from 1875 to 1972 on water levels for Lake Huron. What can we say about the trend? Let’s estimate a couple and see!"
      ],
      "metadata": {
        "id": "TxxhCXax0dIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "huron_linear <- tslm(LakeHuron ~ trend) ## Fit linear trend \n",
        "huron_quad <- tslm(LakeHuron ~ trend + I(trend^2)) ## Fit quadratic trend \n",
        "huron_ma7 <- ma(LakeHuron, order=7) # Fit a q=3 moving average (ma=2q+1)\n",
        "\n",
        "# Put all of these together\n",
        "LakeHuron_with_fits <- cbind(LakeHuron, \n",
        "                             Linear_trend=fitted(huron_linear), \n",
        "                             Quadratic_trend=fitted(huron_quad),\n",
        "                             Moving_avg7 = huron_ma7) \n",
        "                             \n",
        "# Construct the plot \n",
        "autoplot(LakeHuron_with_fits) + \n",
        "  ylab(\"Water Level (in feet)\") + xlab(\"Year\") +\n",
        "  ggtitle(\"Lake Huron water levels (1875-1972)\") +  \n",
        "  guides(colour= guide_legend(title = \"Data series\")) + \n",
        "  scale_colour_manual(values=c(\"black\",\"red\",\"blue\",\"yellow\"))  "
      ],
      "metadata": {
        "id": "7yu29k8D0cgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's look at the residuals.\n",
        "Remember that after removing the trend, the result should somehow look like a stationary process or at least like a mean-zero process!"
      ],
      "metadata": {
        "id": "dZz5FGKb5iau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract residuals for each estimated trend\n",
        "LakeHuron_resids<-cbind(Res_orig=LakeHuron-mean(LakeHuron),\n",
        "                        Res_linear=LakeHuron-fitted(huron_linear),\n",
        "                        Res_quad=LakeHuron-fitted(huron_quad), \n",
        "                        Res_MA7 = LakeHuron - huron_ma7)\n",
        "\n",
        "# produce the autoplot \n",
        "autoplot(LakeHuron_resids, facet=TRUE) + xlab(\"Year\") + ylab(\"Residuals\") +  \n",
        "  ggtitle(\"Lake Huron water levels (1875-1972) Residuals\") + \n",
        "  geom_hline(yintercept = 0) + \n",
        "  guides(colour=guide_legend(title=\"Data Series\"))"
      ],
      "metadata": {
        "id": "Q0SPinch16zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that for all trends, they definitely look a lot more mean-zero than the original one, and the MA7 (order-7 moving-average) looks a bit more like White Noise than the others. You might also notice that we lost some data along the process: indeed, as we can only go over a window [-q,q], we will lose a couple of observations at the borders. We can inspect this more formally using the ACF plots:"
      ],
      "metadata": {
        "id": "gaTfi87A565V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ggAcf(LakeHuron) + ggtitle(\"ACF for original series\") + ylim(c(-1,1))\n",
        "ggAcf(LakeHuron_resids[,\"Res_linear\"]) + ggtitle(\"ACF removing linear trend\") + ylim(c(-1,1)) \n",
        "ggAcf(LakeHuron_resids[,\"Res_quad\"]) + ggtitle(\"ACF removing quadratic trend\") + ylim(c(-1,1)) \n",
        "ggAcf(LakeHuron_resids[,\"Res_MA7\"]) + ggtitle(\"ACF removing MA7 trend\")+ ylim(c(-1,1))"
      ],
      "metadata": {
        "id": "naiuQIkl29v6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indeed, all of our trend estimations do significantly much better than the original series, as more lags fall within the confidence bounds. However, we see that sometimes over-estimating the trend can lead to further correlation. Overall, the MA7 seems to be the best choice. The drawback is that by employing an MA filter, we sacrifice some data, and therefore, some predictive power. In the end, it is up to you to make these choices over which trend estimation to use over another!"
      ],
      "metadata": {
        "id": "YBwmFJlW6Bb-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://medium.com/analytics-vidhya/a-complete-introduction-to-time-series-analysis-with-r-classical-decomposition-model-a4548a0c99b9"
      ],
      "metadata": {
        "id": "uKMUL5j64EeS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Estimating Seasonality\n",
        "\n",
        "In this example, we consider the anti-diabetic drugs sale data, which can be found in the package fpp2 :\n",
        "\n",
        "Our aim os to estimate the seasonal component and obtain deseasonalized data dt."
      ],
      "metadata": {
        "id": "TxBuC3qV5yjo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "autoplot(a10) + \n",
        "  ggtitle(\"Antidiabetic drug sales\") + \n",
        "  ylab(\"$ million\") + \n",
        "  xlab(\"Year\")"
      ],
      "metadata": {
        "id": "Ich-WXab8LTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is clear that not only there is a trend, but also some seasonal component that repeats every year, when the sales suddenly spike. We can estimate the trend with the ma function (that is, we are fitting a moving-average) and also the ses function, for exponential smoothing with alpha=0.2."
      ],
      "metadata": {
        "id": "VQ0grTv4890v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# obtain the fitted values\n",
        "diab_ma5 <- ma(a10,5) # order 5 moving average\n",
        "diab_ses <- ses(a10,alpha=0.2) # esponential smoothing\n",
        "diab_with_fits <- cbind(Orig=a10, MA5=diab_ma5, ES=fitted(diab_ses)) \n",
        "\n",
        "# construct the autplot \n",
        "autoplot(diab_with_fits, facet=TRUE)"
      ],
      "metadata": {
        "id": "x_uY8Rhb-mxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that these average estimates also capture some of the seasonality. How do we know which one is better? We can inspect the residuals:"
      ],
      "metadata": {
        "id": "4XmgVkDO-vi6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Obtain the residual plots\n",
        "diab_resids <- cbind(Resid_Orig = a10 - mean(a10), \n",
        "                     Resid_MA5 = a10 - diab_ma5, \n",
        "                     Resid_ses = a10 - fitted(diab_ses))\n",
        "\n",
        "# construct a plot \n",
        "autoplot(diab_resids,facet=TRUE)"
      ],
      "metadata": {
        "id": "-QDXf8J3-x8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see from the residuals that the exponential smoothing was perhaps a bit too much of an overkill, and overall the MA5 residuals look closer to White Noise. Now, our task is to estimate seasonality: for this sake, we will obtain the frequency of the data observations (that is, the number of seasons), and plot what this seasonality looks like:"
      ],
      "metadata": {
        "id": "cwTccrHT_C9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# frequency givs number of seasons\n",
        "frequency(diab_resids[,\"Resid_MA5\"]) \n",
        "\n",
        "\n",
        "# Fit a linear model for the diabetes season \n",
        "diab_seasonlm <- tslm(Resid_MA5 ~ season, data=diab_resids) \n",
        "autoplot(fitted(diab_seasonlm))"
      ],
      "metadata": {
        "id": "S3JNrr1v_CWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that indeed the peaks are exactly every 12 seasons (that is, every 12 months= a year). We can even obtain the coefficients for this model:"
      ],
      "metadata": {
        "id": "FDWLSYYVBJuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain a table of the values at each component of the season\n",
        "kable(coef(diab_seasonlm))"
      ],
      "metadata": {
        "id": "74hWG2WWBMmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we remove this seasonal component, and hope that the result looks somehow like a White Noise process:"
      ],
      "metadata": {
        "id": "HsHP32AhBSFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Bind all the objects with substracted trend and seasonality\n",
        "diab_resids <- cbind(Resid_Orig = a10 - mean(a10), \n",
        "                     Resid_MA5 = a10 - diab_ma5, \n",
        "                     Resid_ses = a10 - fitted(diab_ses), \n",
        "                     Resid_MA5_season = (a10 - diab_ma5) - fitted(diab_seasonlm))\n",
        "\n",
        "# plot of the MA5 residuals with and without the seasonal component. \n",
        "autoplot(diab_resids[,c(\"Resid_MA5\",\"Resid_MA5_season\")],facet=TRUE) "
      ],
      "metadata": {
        "id": "5mP4rf76BTy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The second graph here shows the residuals after removing the seasonal component. Don’t be fooled by the shape! Note that the range goes (mostly) from -2.5 to 2, as opposed to -3.5 to almost 6 in the MA5 residuals. Except for the last couple of years, it looks like the process is closer to White Noise. Let’s inspect the ACF plots:"
      ],
      "metadata": {
        "id": "gdPMeYcFB3Zo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plot ACF plot without moving average , but trend present. \n",
        "ggAcf(diab_resids[,\"Resid_MA5\"]) + ylim(c(-1,1))\n",
        "\n",
        "# plot ACF plot without moving average , but trend present. \n",
        "ggAcf(diab_resids[,\"Resid_MA5_season\"]) + ylim(c(-1,1))"
      ],
      "metadata": {
        "id": "u-rwIW99B9AA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The residual definitely look better for the detrended and deseasonalized series. For the sake of illustration and as a summary, we can compare the original data against the deseasonalized data only, as well as the detrended and deseasonalized data residuals:"
      ],
      "metadata": {
        "id": "-9_OZRpTBMEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# form deseasonalized data \n",
        "diab_deseason <- a10 - mean(a10) - fitted(diab_seasonlm) \n",
        "\n",
        "# Re-estimate the trend from the deseasonalized data (using MA5), and substract it from the deseasonalized data \n",
        "diab_deseason_detrend_MA5 <- diab_deseason - ma(diab_deseason, 5) \n",
        "\n",
        "# Create cbind of residuals  for different components \n",
        "diab_deseason_resids <- cbind(Orig=a10 - mean(a10),  # original=detrended \n",
        "                   Deseason= diab_deseason, \n",
        "                   Deseason_detrend = diab_deseason_detrend_MA5) \n",
        "\n",
        "# Plot the different data \n",
        "autoplot(diab_deseason_resids, facet=TRUE)"
      ],
      "metadata": {
        "id": "W_5n_4p6COjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That’s quite some progress! You might still be wondering “that’s awesome, but why are we doing this?”. We will answer that question in-depth much later, but the spoiler is the following: if we can obtain good, approximately White Noise residuals, we can fit better models, and then simply add back the trend and seasonal components at prediction time :)\n",
        "\n",
        "https://medium.com/analytics-vidhya/a-complete-introduction-to-time-series-analysis-with-r-classical-decomposition-model-part-ii-aa43b524680d"
      ],
      "metadata": {
        "id": "o10kX_VLCedB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ARIMA\n",
        "ARIMA is an acronym for Auto Regressive (AR) Integrated (I) Moving Average (MA) which indicates that an ARIMA model has three components to it.\n",
        "\n",
        "ARIMA models can be expressed in two forms: Non-seasonal models where the model exhibits an order in the form of (p,d,q) where:\n",
        "\n",
        "p = The order of the Auto Regressive Model\n",
        "\n",
        "d = The order of differencing\n",
        "\n",
        "q = The order of the Moving Average\n",
        "\n",
        "###Auto Regressive Models (AR):\n",
        "\n",
        "Auto regressive models are similar to a regression model but the regressor in this case is the same dependent variable with a specific lag.\n",
        "\n",
        "###Differencing (I):\n",
        "\n",
        "For ARIMA to perform at its best it needs the data to be stationary. That means that the mean and variance are constant over the entire set. Differencing is used to transform the data so that it is stationary.\n",
        "\n",
        "###Moving Average (MA):\n",
        "\n",
        "Moving averages are widely used in time series analysis and is an already well-known concept. It involves getting the average of the points in a series for a specific lag.\n",
        "\n",
        "###Seasonal ARIMA models (SARIMA):\n",
        "\n",
        "These models take into account the seasonality in the data and does the same ARIMA steps but on the seasonal pattern. So, if the data has a seasonal pattern every quarter then the SARIMA will get an order for (p,d,q) for all the points and a (P,D,Q) for each quarter.\n",
        "\n",
        "###Regression Models VS. ARIMA:\n",
        "\n",
        "Now comes the real deal. ARIMA models are great for forecasting blindly into the future using historical data. However, sometimes we don’t need to forecast blindly, sometimes we have variables that can help us predict future behavior. Regression models are ideal in this scenario."
      ],
      "metadata": {
        "id": "2R3dnT1OvJAM"
      }
    }
  ]
}